
==> Audit <==
|------------|-----------------|----------|------------------|---------|---------------------|---------------------|
|  Command   |      Args       | Profile  |       User       | Version |     Start Time      |      End Time       |
|------------|-----------------|----------|------------------|---------|---------------------|---------------------|
| start      | --driver=hyperv | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:03 +06 |                     |
| start      | --driver=hyperv | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:05 +06 |                     |
| docker-env |                 | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:06 +06 |                     |
| start      | --driver=docker | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:07 +06 |                     |
| start      | --driver=hyperv | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:08 +06 |                     |
| start      | --driver=docker | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:08 +06 |                     |
| start      |                 | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:08 +06 |                     |
| docker-env |                 | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:13 +06 |                     |
| docker-env |                 | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:14 +06 |                     |
| docker-env |                 | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:16 +06 |                     |
| addons     | enable ingress  | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:16 +06 |                     |
| delete     | --all           | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:17 +06 | 15 Feb 25 23:17 +06 |
| start      | --driver=docker | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:17 +06 | 15 Feb 25 23:20 +06 |
| addons     | enable ingress  | minikube | MHRISHAD-D\mahab | v1.35.0 | 15 Feb 25 23:21 +06 | 15 Feb 25 23:23 +06 |
|------------|-----------------|----------|------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/02/15 23:17:36
Running on machine: MHRISHAD-D
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0215 23:17:36.040380   15576 out.go:345] Setting OutFile to fd 104 ...
I0215 23:17:36.041442   15576 out.go:397] isatty.IsTerminal(104) = true
I0215 23:17:36.041442   15576 out.go:358] Setting ErrFile to fd 112...
I0215 23:17:36.041442   15576 out.go:397] isatty.IsTerminal(112) = true
I0215 23:17:36.054925   15576 out.go:352] Setting JSON to false
I0215 23:17:36.060476   15576 start.go:129] hostinfo: {"hostname":"MHRISHAD-D","uptime":1468,"bootTime":1739638387,"procs":253,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3194 Build 26100.3194","kernelVersion":"10.0.26100.3194 Build 26100.3194","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"6001c6a1-082f-4928-906e-9d8160be4c95"}
W0215 23:17:36.060476   15576 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0215 23:17:36.067525   15576 out.go:177] 😄  minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.26100.3194 Build 26100.3194
I0215 23:17:36.074985   15576 notify.go:220] Checking for updates...
I0215 23:17:36.075491   15576 driver.go:394] Setting default libvirt URI to qemu:///system
I0215 23:17:36.154111   15576 docker.go:123] docker version: linux-27.1.1:Docker Desktop 4.33.1 (161083)
I0215 23:17:36.159449   15576 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0215 23:17:36.933808   15576 info.go:266] docker info: {ID:dde9d507-6013-4528-bb24-106d5b2dd40e Containers:48 ContainersRunning:46 ContainersPaused:0 ContainersStopped:2 Images:17 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:205 OomKillDisable:true NGoroutines:189 SystemTime:2025-02-15 17:17:36.89483887 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8020418560 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0215 23:17:36.937016   15576 out.go:177] ✨  Using the docker driver based on user configuration
I0215 23:17:36.942192   15576 start.go:297] selected driver: docker
I0215 23:17:36.942192   15576 start.go:901] validating driver "docker" against <nil>
I0215 23:17:36.942192   15576 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0215 23:17:36.958093   15576 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0215 23:17:37.394117   15576 info.go:266] docker info: {ID:dde9d507-6013-4528-bb24-106d5b2dd40e Containers:48 ContainersRunning:46 ContainersPaused:0 ContainersStopped:2 Images:17 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:205 OomKillDisable:true NGoroutines:189 SystemTime:2025-02-15 17:17:37.373115157 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8020418560 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0215 23:17:37.394657   15576 start_flags.go:310] no existing cluster config was found, will generate one from the flags 
I0215 23:17:37.529911   15576 start_flags.go:393] Using suggested 3900MB memory alloc based on sys=15781MB, container=7648MB
I0215 23:17:37.530957   15576 start_flags.go:929] Wait components to verify : map[apiserver:true system_pods:true]
I0215 23:17:37.535262   15576 out.go:177] 📌  Using Docker Desktop driver with root privileges
I0215 23:17:37.543625   15576 cni.go:84] Creating CNI manager for ""
I0215 23:17:37.543625   15576 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0215 23:17:37.543625   15576 start_flags.go:319] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I0215 23:17:37.544137   15576 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mahab:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0215 23:17:37.548600   15576 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0215 23:17:37.554939   15576 cache.go:121] Beginning downloading kic base image for docker with docker
I0215 23:17:37.559906   15576 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0215 23:17:37.564608   15576 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0215 23:17:37.564608   15576 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0215 23:17:37.564608   15576 preload.go:146] Found local preload: C:\Users\mahab\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0215 23:17:37.564608   15576 cache.go:56] Caching tarball of preloaded images
I0215 23:17:37.570288   15576 preload.go:172] Found C:\Users\mahab\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0215 23:17:37.580413   15576 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0215 23:17:37.581718   15576 profile.go:143] Saving config to C:\Users\mahab\.minikube\profiles\minikube\config.json ...
I0215 23:17:37.581718   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\config.json: {Name:mk99db64a015e6987a26a6df0b7b8d876e9f1dbc Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:17:37.674787   15576 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0215 23:17:37.674787   15576 localpath.go:146] windows sanitize: C:\Users\mahab\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\mahab\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0215 23:17:37.675304   15576 localpath.go:146] windows sanitize: C:\Users\mahab\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\mahab\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0215 23:17:37.675304   15576 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0215 23:17:37.675834   15576 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0215 23:18:26.724700   15576 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0215 23:18:26.724700   15576 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0215 23:18:26.724700   15576 localpath.go:146] windows sanitize: C:\Users\mahab\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\mahab\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0215 23:19:49.957928   15576 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0215 23:19:49.959022   15576 cache.go:227] Successfully downloaded all kic artifacts
I0215 23:19:49.959612   15576 start.go:360] acquireMachinesLock for minikube: {Name:mk39567d952773f8f155b36d6dfdfa6746a45577 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0215 23:19:49.959612   15576 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0215 23:19:49.960174   15576 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mahab:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0215 23:19:49.961187   15576 start.go:125] createHost starting for "" (driver="docker")
I0215 23:19:49.968139   15576 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=3900MB) ...
I0215 23:19:50.042532   15576 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0215 23:19:50.042532   15576 client.go:168] LocalClient.Create starting
I0215 23:19:50.042532   15576 main.go:141] libmachine: Reading certificate data from C:\Users\mahab\.minikube\certs\ca.pem
I0215 23:19:50.043330   15576 main.go:141] libmachine: Decoding PEM data...
I0215 23:19:50.043330   15576 main.go:141] libmachine: Parsing certificate...
I0215 23:19:50.043330   15576 main.go:141] libmachine: Reading certificate data from C:\Users\mahab\.minikube\certs\cert.pem
I0215 23:19:50.043876   15576 main.go:141] libmachine: Decoding PEM data...
I0215 23:19:50.043876   15576 main.go:141] libmachine: Parsing certificate...
I0215 23:19:50.078255   15576 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0215 23:19:50.447708   15576 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0215 23:19:50.469335   15576 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0215 23:19:50.469335   15576 cli_runner.go:164] Run: docker network inspect minikube
W0215 23:19:50.624139   15576 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0215 23:19:50.624139   15576 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0215 23:19:50.624139   15576 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0215 23:19:50.656286   15576 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0215 23:19:50.943796   15576 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001fb3410}
I0215 23:19:50.944334   15576 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0215 23:19:50.955958   15576 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0215 23:19:51.332942   15576 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0215 23:19:51.336019   15576 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0215 23:19:51.373742   15576 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0215 23:19:51.556075   15576 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0215 23:19:51.691741   15576 oci.go:103] Successfully created a docker volume minikube
I0215 23:19:51.699023   15576 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0215 23:19:54.167095   15576 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (2.4680714s)
I0215 23:19:54.167095   15576 oci.go:107] Successfully prepared a docker volume minikube
I0215 23:19:54.167095   15576 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0215 23:19:54.167095   15576 kic.go:194] Starting extracting preloaded images to volume ...
I0215 23:19:54.180521   15576 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\mahab\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0215 23:20:16.632927   15576 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\mahab\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (22.4518264s)
I0215 23:20:16.632927   15576 kic.go:203] duration metric: took 22.4658325s to extract preloaded images to volume ...
I0215 23:20:16.639360   15576 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0215 23:20:17.435059   15576 info.go:266] docker info: {ID:dde9d507-6013-4528-bb24-106d5b2dd40e Containers:48 ContainersRunning:46 ContainersPaused:0 ContainersStopped:2 Images:18 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:210 OomKillDisable:true NGoroutines:193 SystemTime:2025-02-15 17:20:17.377780505 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8020418560 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41 Expected:2bf793ef6dc9a18e00cb12efb64355c2c9d5eb41} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.16.1-desktop.1] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.29.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.34] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Alpha) Vendor:Docker Inc. Version:v0.0.14] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.25] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.3.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.11.0]] Warnings:<nil>}}
I0215 23:20:17.451541   15576 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0215 23:20:18.063704   15576 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb --memory-swap=3900mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0215 23:20:19.402257   15576 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3900mb --memory-swap=3900mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279: (1.3380177s)
I0215 23:20:19.417089   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0215 23:20:19.600766   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0215 23:20:19.777382   15576 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0215 23:20:19.991242   15576 oci.go:144] the created container "minikube" has a running status.
I0215 23:20:19.991242   15576 kic.go:225] Creating ssh key for kic: C:\Users\mahab\.minikube\machines\minikube\id_rsa...
I0215 23:20:20.388368   15576 kic_runner.go:191] docker (temp): C:\Users\mahab\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0215 23:20:20.660432   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0215 23:20:20.795558   15576 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0215 23:20:20.795558   15576 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0215 23:20:21.069116   15576 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\mahab\.minikube\machines\minikube\id_rsa...
I0215 23:20:21.927581   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0215 23:20:22.019091   15576 machine.go:93] provisionDockerMachine start ...
I0215 23:20:22.027986   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:22.123251   15576 main.go:141] libmachine: Using SSH client type: native
I0215 23:20:22.125450   15576 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe85360] 0xe87ea0 <nil>  [] 0s} 127.0.0.1 52467 <nil> <nil>}
I0215 23:20:22.125450   15576 main.go:141] libmachine: About to run SSH command:
hostname
I0215 23:20:22.297386   15576 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0215 23:20:22.299079   15576 ubuntu.go:169] provisioning hostname "minikube"
I0215 23:20:22.306329   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:22.385604   15576 main.go:141] libmachine: Using SSH client type: native
I0215 23:20:22.385604   15576 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe85360] 0xe87ea0 <nil>  [] 0s} 127.0.0.1 52467 <nil> <nil>}
I0215 23:20:22.386132   15576 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0215 23:20:22.578687   15576 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0215 23:20:22.587198   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:22.663728   15576 main.go:141] libmachine: Using SSH client type: native
I0215 23:20:22.664949   15576 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe85360] 0xe87ea0 <nil>  [] 0s} 127.0.0.1 52467 <nil> <nil>}
I0215 23:20:22.665372   15576 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0215 23:20:22.835264   15576 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0215 23:20:22.836335   15576 ubuntu.go:175] set auth options {CertDir:C:\Users\mahab\.minikube CaCertPath:C:\Users\mahab\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\mahab\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\mahab\.minikube\machines\server.pem ServerKeyPath:C:\Users\mahab\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\mahab\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\mahab\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\mahab\.minikube}
I0215 23:20:22.836335   15576 ubuntu.go:177] setting up certificates
I0215 23:20:22.836335   15576 provision.go:84] configureAuth start
I0215 23:20:22.842325   15576 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0215 23:20:22.921992   15576 provision.go:143] copyHostCerts
I0215 23:20:22.922526   15576 exec_runner.go:151] cp: C:\Users\mahab\.minikube\certs\ca.pem --> C:\Users\mahab\.minikube/ca.pem (1074 bytes)
I0215 23:20:22.923611   15576 exec_runner.go:151] cp: C:\Users\mahab\.minikube\certs\cert.pem --> C:\Users\mahab\.minikube/cert.pem (1119 bytes)
I0215 23:20:22.924160   15576 exec_runner.go:151] cp: C:\Users\mahab\.minikube\certs\key.pem --> C:\Users\mahab\.minikube/key.pem (1675 bytes)
I0215 23:20:22.925260   15576 provision.go:117] generating server cert: C:\Users\mahab\.minikube\machines\server.pem ca-key=C:\Users\mahab\.minikube\certs\ca.pem private-key=C:\Users\mahab\.minikube\certs\ca-key.pem org=mahab.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0215 23:20:23.438277   15576 provision.go:177] copyRemoteCerts
I0215 23:20:23.439283   15576 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0215 23:20:23.445802   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:23.526134   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
I0215 23:20:23.641035   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0215 23:20:23.673098   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0215 23:20:23.712165   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0215 23:20:23.752271   15576 provision.go:87] duration metric: took 913.7663ms to configureAuth
I0215 23:20:23.752271   15576 ubuntu.go:193] setting minikube options for container-runtime
I0215 23:20:23.753334   15576 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0215 23:20:23.760394   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:23.855111   15576 main.go:141] libmachine: Using SSH client type: native
I0215 23:20:23.855656   15576 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe85360] 0xe87ea0 <nil>  [] 0s} 127.0.0.1 52467 <nil> <nil>}
I0215 23:20:23.855656   15576 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0215 23:20:24.016489   15576 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0215 23:20:24.016489   15576 ubuntu.go:71] root file system type: overlay
I0215 23:20:24.016489   15576 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0215 23:20:24.023898   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:24.110060   15576 main.go:141] libmachine: Using SSH client type: native
I0215 23:20:24.110599   15576 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe85360] 0xe87ea0 <nil>  [] 0s} 127.0.0.1 52467 <nil> <nil>}
I0215 23:20:24.110599   15576 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0215 23:20:24.310757   15576 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0215 23:20:24.321425   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:24.403554   15576 main.go:141] libmachine: Using SSH client type: native
I0215 23:20:24.403554   15576 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xe85360] 0xe87ea0 <nil>  [] 0s} 127.0.0.1 52467 <nil> <nil>}
I0215 23:20:24.403554   15576 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0215 23:20:26.028741   15576 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-02-15 17:20:24.292343714 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0215 23:20:26.028741   15576 machine.go:96] duration metric: took 4.0096503s to provisionDockerMachine
I0215 23:20:26.028741   15576 client.go:171] duration metric: took 35.986209s to LocalClient.Create
I0215 23:20:26.028741   15576 start.go:167] duration metric: took 35.986209s to libmachine.API.Create "minikube"
I0215 23:20:26.029279   15576 start.go:293] postStartSetup for "minikube" (driver="docker")
I0215 23:20:26.029279   15576 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0215 23:20:26.030356   15576 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0215 23:20:26.038808   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:26.123431   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
I0215 23:20:26.271097   15576 ssh_runner.go:195] Run: cat /etc/os-release
I0215 23:20:26.279579   15576 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0215 23:20:26.280210   15576 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0215 23:20:26.280210   15576 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0215 23:20:26.280210   15576 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0215 23:20:26.280750   15576 filesync.go:126] Scanning C:\Users\mahab\.minikube\addons for local assets ...
I0215 23:20:26.281294   15576 filesync.go:126] Scanning C:\Users\mahab\.minikube\files for local assets ...
I0215 23:20:26.281839   15576 start.go:296] duration metric: took 252.5599ms for postStartSetup
I0215 23:20:26.293254   15576 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0215 23:20:26.389650   15576 profile.go:143] Saving config to C:\Users\mahab\.minikube\profiles\minikube\config.json ...
I0215 23:20:26.410495   15576 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0215 23:20:26.419705   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:26.525374   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
I0215 23:20:26.656830   15576 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0215 23:20:26.666654   15576 start.go:128] duration metric: took 36.7054673s to createHost
I0215 23:20:26.666654   15576 start.go:83] releasing machines lock for "minikube", held for 36.7070424s
I0215 23:20:26.675657   15576 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0215 23:20:26.760573   15576 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0215 23:20:26.770300   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:26.776778   15576 ssh_runner.go:195] Run: cat /version.json
I0215 23:20:26.784161   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:26.864484   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
I0215 23:20:26.873789   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
W0215 23:20:27.016546   15576 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0215 23:20:27.055234   15576 ssh_runner.go:195] Run: systemctl --version
I0215 23:20:27.082382   15576 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0215 23:20:27.093611   15576 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0215 23:20:27.111634   15576 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0215 23:20:27.112735   15576 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0215 23:20:27.181449   15576 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0215 23:20:27.181449   15576 start.go:495] detecting cgroup driver to use...
I0215 23:20:27.181449   15576 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0215 23:20:27.183670   15576 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0215 23:20:27.237568   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0215 23:20:27.276272   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0215 23:20:27.305263   15576 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0215 23:20:27.330719   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0215 23:20:27.375500   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0215 23:20:27.407775   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0215 23:20:27.463077   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0215 23:20:27.502739   15576 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0215 23:20:27.541200   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0215 23:20:27.578999   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0215 23:20:27.621663   15576 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0215 23:20:27.651385   15576 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0215 23:20:27.668741   15576 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0215 23:20:27.687661   15576 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0215 23:20:27.797786   15576 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0215 23:20:27.798327   15576 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0215 23:20:27.885081   15576 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0215 23:20:28.077262   15576 start.go:495] detecting cgroup driver to use...
I0215 23:20:28.077825   15576 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0215 23:20:28.079634   15576 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0215 23:20:28.113468   15576 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0215 23:20:28.114575   15576 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0215 23:20:28.153435   15576 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0215 23:20:28.201663   15576 ssh_runner.go:195] Run: which cri-dockerd
I0215 23:20:28.214140   15576 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0215 23:20:28.248989   15576 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0215 23:20:28.329166   15576 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0215 23:20:28.582865   15576 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0215 23:20:28.774783   15576 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0215 23:20:28.774783   15576 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0215 23:20:28.819090   15576 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0215 23:20:28.978981   15576 ssh_runner.go:195] Run: sudo systemctl restart docker
I0215 23:20:29.775920   15576 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0215 23:20:29.799872   15576 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0215 23:20:29.818717   15576 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0215 23:20:30.015392   15576 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0215 23:20:30.191811   15576 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0215 23:20:30.356010   15576 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0215 23:20:30.392617   15576 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0215 23:20:30.412280   15576 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0215 23:20:30.532120   15576 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0215 23:20:30.636180   15576 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0215 23:20:30.650915   15576 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0215 23:20:30.657044   15576 start.go:563] Will wait 60s for crictl version
I0215 23:20:30.671777   15576 ssh_runner.go:195] Run: which crictl
I0215 23:20:30.680011   15576 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0215 23:20:30.719828   15576 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0215 23:20:30.726252   15576 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0215 23:20:30.765202   15576 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0215 23:20:30.815680   15576 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0215 23:20:30.823880   15576 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0215 23:20:30.994555   15576 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0215 23:20:31.013072   15576 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0215 23:20:31.019566   15576 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0215 23:20:31.046692   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0215 23:20:31.127563   15576 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mahab:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0215 23:20:31.128107   15576 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0215 23:20:31.134429   15576 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0215 23:20:31.163149   15576 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0215 23:20:31.163149   15576 docker.go:619] Images already preloaded, skipping extraction
I0215 23:20:31.178024   15576 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0215 23:20:31.205952   15576 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0215 23:20:31.206486   15576 cache_images.go:84] Images are preloaded, skipping loading
I0215 23:20:31.206486   15576 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0215 23:20:31.208087   15576 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0215 23:20:31.214529   15576 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0215 23:20:31.289030   15576 cni.go:84] Creating CNI manager for ""
I0215 23:20:31.289030   15576 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0215 23:20:31.289030   15576 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0215 23:20:31.289030   15576 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0215 23:20:31.289568   15576 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0215 23:20:31.290646   15576 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0215 23:20:31.305118   15576 binaries.go:44] Found k8s binaries, skipping transfer
I0215 23:20:31.305640   15576 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0215 23:20:31.320817   15576 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0215 23:20:31.350889   15576 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0215 23:20:31.384712   15576 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0215 23:20:31.424073   15576 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0215 23:20:31.430293   15576 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0215 23:20:31.446149   15576 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0215 23:20:31.605474   15576 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0215 23:20:31.629452   15576 certs.go:68] Setting up C:\Users\mahab\.minikube\profiles\minikube for IP: 192.168.49.2
I0215 23:20:31.629452   15576 certs.go:194] generating shared ca certs ...
I0215 23:20:31.629452   15576 certs.go:226] acquiring lock for ca certs: {Name:mk8715aa862c984115e10b10d005947d479805fd Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:31.630034   15576 certs.go:240] generating "minikubeCA" ca cert: C:\Users\mahab\.minikube\ca.key
I0215 23:20:31.786284   15576 crypto.go:156] Writing cert to C:\Users\mahab\.minikube\ca.crt ...
I0215 23:20:31.786284   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\ca.crt: {Name:mk10ff25ea440c582119c053a5b8f954236bbc2f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:31.786284   15576 crypto.go:164] Writing key to C:\Users\mahab\.minikube\ca.key ...
I0215 23:20:31.786284   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\ca.key: {Name:mkc97e011ce1c3c061f878bcf89ad8c38aa8fc8b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:31.788176   15576 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\mahab\.minikube\proxy-client-ca.key
I0215 23:20:31.962937   15576 crypto.go:156] Writing cert to C:\Users\mahab\.minikube\proxy-client-ca.crt ...
I0215 23:20:31.962937   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\proxy-client-ca.crt: {Name:mk8218228c33ccb9975634ad89cfdb3f38aa0bac Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:31.965024   15576 crypto.go:164] Writing key to C:\Users\mahab\.minikube\proxy-client-ca.key ...
I0215 23:20:31.965024   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\proxy-client-ca.key: {Name:mk845ecc8af04df4bf019e33e3f3fb4f52294161 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:31.965024   15576 certs.go:256] generating profile certs ...
I0215 23:20:31.965024   15576 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\mahab\.minikube\profiles\minikube\client.key
I0215 23:20:31.967533   15576 crypto.go:68] Generating cert C:\Users\mahab\.minikube\profiles\minikube\client.crt with IP's: []
I0215 23:20:32.881056   15576 crypto.go:156] Writing cert to C:\Users\mahab\.minikube\profiles\minikube\client.crt ...
I0215 23:20:32.881056   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\client.crt: {Name:mkb445a3d1a8fe856f8a282d0c2f41fe8526f9b9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:32.883073   15576 crypto.go:164] Writing key to C:\Users\mahab\.minikube\profiles\minikube\client.key ...
I0215 23:20:32.883073   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\client.key: {Name:mk2ce6fcfbbbee9d497c15799606783d3e0fe233 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:32.883073   15576 certs.go:363] generating signed profile cert for "minikube": C:\Users\mahab\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0215 23:20:32.883073   15576 crypto.go:68] Generating cert C:\Users\mahab\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0215 23:20:33.271718   15576 crypto.go:156] Writing cert to C:\Users\mahab\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0215 23:20:33.271718   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mk41f96df11ab057a83df52ca8078eef52cf2c4d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:33.271718   15576 crypto.go:164] Writing key to C:\Users\mahab\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0215 23:20:33.271718   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mk9b18722b6af86a2546960d3329d604099da9b1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:33.273738   15576 certs.go:381] copying C:\Users\mahab\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\mahab\.minikube\profiles\minikube\apiserver.crt
I0215 23:20:33.279906   15576 certs.go:385] copying C:\Users\mahab\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\mahab\.minikube\profiles\minikube\apiserver.key
I0215 23:20:33.284091   15576 certs.go:363] generating signed profile cert for "aggregator": C:\Users\mahab\.minikube\profiles\minikube\proxy-client.key
I0215 23:20:33.284091   15576 crypto.go:68] Generating cert C:\Users\mahab\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0215 23:20:34.033926   15576 crypto.go:156] Writing cert to C:\Users\mahab\.minikube\profiles\minikube\proxy-client.crt ...
I0215 23:20:34.033926   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\proxy-client.crt: {Name:mkdb52853c36ddb678c659d6c4b3c3d6cd02d60b Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:34.034943   15576 crypto.go:164] Writing key to C:\Users\mahab\.minikube\profiles\minikube\proxy-client.key ...
I0215 23:20:34.034943   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.minikube\profiles\minikube\proxy-client.key: {Name:mk43f4121fa58c6110beaef3bfd65fd6c605b271 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:34.039467   15576 certs.go:484] found cert: C:\Users\mahab\.minikube\certs\ca-key.pem (1679 bytes)
I0215 23:20:34.039467   15576 certs.go:484] found cert: C:\Users\mahab\.minikube\certs\ca.pem (1074 bytes)
I0215 23:20:34.039467   15576 certs.go:484] found cert: C:\Users\mahab\.minikube\certs\cert.pem (1119 bytes)
I0215 23:20:34.039467   15576 certs.go:484] found cert: C:\Users\mahab\.minikube\certs\key.pem (1675 bytes)
I0215 23:20:34.046216   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0215 23:20:34.086113   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0215 23:20:34.123644   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0215 23:20:34.155160   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0215 23:20:34.196111   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0215 23:20:34.233355   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0215 23:20:34.271782   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0215 23:20:34.308250   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0215 23:20:34.342247   15576 ssh_runner.go:362] scp C:\Users\mahab\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0215 23:20:34.392501   15576 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0215 23:20:34.434069   15576 ssh_runner.go:195] Run: openssl version
I0215 23:20:34.445082   15576 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0215 23:20:34.475116   15576 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0215 23:20:34.483508   15576 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Feb 15 17:20 /usr/share/ca-certificates/minikubeCA.pem
I0215 23:20:34.497235   15576 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0215 23:20:34.507347   15576 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0215 23:20:34.533819   15576 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0215 23:20:34.540476   15576 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0215 23:20:34.540476   15576 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\mahab:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0215 23:20:34.551002   15576 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0215 23:20:34.586774   15576 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0215 23:20:34.607760   15576 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0215 23:20:34.632492   15576 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0215 23:20:34.633001   15576 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0215 23:20:34.654840   15576 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0215 23:20:34.654840   15576 kubeadm.go:157] found existing configuration files:

I0215 23:20:34.655879   15576 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0215 23:20:34.680457   15576 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0215 23:20:34.681026   15576 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0215 23:20:34.756176   15576 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0215 23:20:34.825973   15576 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0215 23:20:34.826960   15576 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0215 23:20:34.857548   15576 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0215 23:20:34.878406   15576 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0215 23:20:34.878937   15576 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0215 23:20:34.898780   15576 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0215 23:20:34.920150   15576 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0215 23:20:34.920698   15576 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0215 23:20:34.946764   15576 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0215 23:20:35.031750   15576 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0215 23:20:35.031750   15576 kubeadm.go:310] [preflight] Running pre-flight checks
I0215 23:20:35.227372   15576 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0215 23:20:35.227372   15576 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0215 23:20:35.227885   15576 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0215 23:20:35.261517   15576 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0215 23:20:35.267481   15576 out.go:235]     ▪ Generating certificates and keys ...
I0215 23:20:35.268049   15576 kubeadm.go:310] [certs] Using existing ca certificate authority
I0215 23:20:35.268595   15576 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0215 23:20:35.534677   15576 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0215 23:20:35.871352   15576 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0215 23:20:35.974754   15576 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0215 23:20:36.204714   15576 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0215 23:20:36.791643   15576 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0215 23:20:36.792434   15576 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0215 23:20:37.721054   15576 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0215 23:20:37.721559   15576 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0215 23:20:38.040868   15576 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0215 23:20:38.277644   15576 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0215 23:20:38.404287   15576 kubeadm.go:310] [certs] Generating "sa" key and public key
I0215 23:20:38.404287   15576 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0215 23:20:38.874174   15576 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0215 23:20:39.178578   15576 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0215 23:20:39.633209   15576 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0215 23:20:40.119219   15576 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0215 23:20:40.464737   15576 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0215 23:20:40.464737   15576 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0215 23:20:40.464737   15576 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0215 23:20:40.469354   15576 out.go:235]     ▪ Booting up control plane ...
I0215 23:20:40.470458   15576 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0215 23:20:40.470458   15576 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0215 23:20:40.470458   15576 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0215 23:20:40.488939   15576 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0215 23:20:40.498135   15576 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0215 23:20:40.498135   15576 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0215 23:20:40.700425   15576 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0215 23:20:40.700425   15576 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0215 23:20:41.197764   15576 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 500.825795ms
I0215 23:20:41.198270   15576 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0215 23:20:49.699102   15576 kubeadm.go:310] [api-check] The API server is healthy after 8.502020365s
I0215 23:20:49.716807   15576 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0215 23:20:49.760915   15576 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0215 23:20:49.813781   15576 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0215 23:20:49.813781   15576 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0215 23:20:49.837252   15576 kubeadm.go:310] [bootstrap-token] Using token: xbb5ke.sjlj9ulzopknvh1i
I0215 23:20:49.839986   15576 out.go:235]     ▪ Configuring RBAC rules ...
I0215 23:20:49.841080   15576 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0215 23:20:49.861641   15576 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0215 23:20:49.872818   15576 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0215 23:20:49.881265   15576 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0215 23:20:49.894104   15576 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0215 23:20:49.904887   15576 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0215 23:20:50.115256   15576 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0215 23:20:50.802293   15576 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0215 23:20:51.118401   15576 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0215 23:20:51.121090   15576 kubeadm.go:310] 
I0215 23:20:51.121090   15576 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0215 23:20:51.121090   15576 kubeadm.go:310] 
I0215 23:20:51.121090   15576 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0215 23:20:51.121090   15576 kubeadm.go:310] 
I0215 23:20:51.121090   15576 kubeadm.go:310]   mkdir -p $HOME/.kube
I0215 23:20:51.121632   15576 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0215 23:20:51.121632   15576 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0215 23:20:51.121632   15576 kubeadm.go:310] 
I0215 23:20:51.121632   15576 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0215 23:20:51.121632   15576 kubeadm.go:310] 
I0215 23:20:51.121632   15576 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0215 23:20:51.121632   15576 kubeadm.go:310] 
I0215 23:20:51.121632   15576 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0215 23:20:51.121632   15576 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0215 23:20:51.121632   15576 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0215 23:20:51.121632   15576 kubeadm.go:310] 
I0215 23:20:51.122173   15576 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0215 23:20:51.122173   15576 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0215 23:20:51.122173   15576 kubeadm.go:310] 
I0215 23:20:51.122173   15576 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token xbb5ke.sjlj9ulzopknvh1i \
I0215 23:20:51.122173   15576 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:84e1408e67a53790203e0a1197726f6066550bb0f8f1588a8d7885372457685b \
I0215 23:20:51.122714   15576 kubeadm.go:310] 	--control-plane 
I0215 23:20:51.122714   15576 kubeadm.go:310] 
I0215 23:20:51.122714   15576 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0215 23:20:51.122714   15576 kubeadm.go:310] 
I0215 23:20:51.122714   15576 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token xbb5ke.sjlj9ulzopknvh1i \
I0215 23:20:51.122714   15576 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:84e1408e67a53790203e0a1197726f6066550bb0f8f1588a8d7885372457685b 
I0215 23:20:51.125425   15576 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0215 23:20:51.125425   15576 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0215 23:20:51.125959   15576 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0215 23:20:51.125959   15576 cni.go:84] Creating CNI manager for ""
I0215 23:20:51.125959   15576 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0215 23:20:51.129743   15576 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0215 23:20:51.137892   15576 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0215 23:20:51.172290   15576 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0215 23:20:51.231077   15576 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0215 23:20:51.231617   15576 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0215 23:20:51.231617   15576 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_02_15T23_20_51_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0215 23:20:51.477656   15576 kubeadm.go:1113] duration metric: took 246.5791ms to wait for elevateKubeSystemPrivileges
I0215 23:20:51.522338   15576 ops.go:34] apiserver oom_adj: -16
I0215 23:20:51.522338   15576 kubeadm.go:394] duration metric: took 16.9818618s to StartCluster
I0215 23:20:51.522338   15576 settings.go:142] acquiring lock: {Name:mk421334244b5e7b590947fead3a7709adabaeef Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:51.522338   15576 settings.go:150] Updating kubeconfig:  C:\Users\mahab\.kube\config
I0215 23:20:51.526749   15576 lock.go:35] WriteFile acquiring C:\Users\mahab\.kube\config: {Name:mkefe8df9517ed19139fde0e6e82b5762a352df4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0215 23:20:51.528400   15576 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0215 23:20:51.530666   15576 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0215 23:20:51.533359   15576 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0215 23:20:51.538564   15576 out.go:177] 🔎  Verifying Kubernetes components...
I0215 23:20:51.536877   15576 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0215 23:20:51.539105   15576 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0215 23:20:51.539105   15576 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0215 23:20:51.541292   15576 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0215 23:20:51.541292   15576 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0215 23:20:51.543462   15576 host.go:66] Checking if "minikube" exists ...
I0215 23:20:51.544554   15576 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0215 23:20:51.563882   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0215 23:20:51.564428   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0215 23:20:51.686936   15576 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0215 23:20:51.690811   15576 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0215 23:20:51.690811   15576 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0215 23:20:51.697331   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:51.700623   15576 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0215 23:20:51.700623   15576 host.go:66] Checking if "minikube" exists ...
I0215 23:20:51.717982   15576 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0215 23:20:51.788154   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
I0215 23:20:51.836294   15576 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0215 23:20:51.836294   15576 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0215 23:20:51.844642   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0215 23:20:51.924217   15576 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0215 23:20:51.924217   15576 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52467 SSHKeyPath:C:\Users\mahab\.minikube\machines\minikube\id_rsa Username:docker}
I0215 23:20:51.939213   15576 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0215 23:20:52.118405   15576 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0215 23:20:52.132705   15576 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0215 23:20:52.719636   15576 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0215 23:20:52.732800   15576 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0215 23:20:52.919037   15576 api_server.go:52] waiting for apiserver process to appear ...
I0215 23:20:52.922454   15576 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0215 23:20:53.274027   15576 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0215 23:20:53.306650   15576 api_server.go:72] duration metric: took 1.7732906s to wait for apiserver process to appear ...
I0215 23:20:53.306650   15576 api_server.go:88] waiting for apiserver healthz status ...
I0215 23:20:53.306650   15576 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.1739449s)
I0215 23:20:53.306650   15576 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52471/healthz ...
I0215 23:20:53.310445   15576 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0215 23:20:53.317766   15576 addons.go:514] duration metric: took 1.7870996s for enable addons: enabled=[default-storageclass storage-provisioner]
I0215 23:20:53.325623   15576 api_server.go:279] https://127.0.0.1:52471/healthz returned 200:
ok
I0215 23:20:53.328853   15576 api_server.go:141] control plane version: v1.32.0
I0215 23:20:53.328853   15576 api_server.go:131] duration metric: took 22.203ms to wait for apiserver health ...
I0215 23:20:53.328853   15576 system_pods.go:43] waiting for kube-system pods to appear ...
I0215 23:20:53.359364   15576 system_pods.go:59] 5 kube-system pods found
I0215 23:20:53.359969   15576 system_pods.go:61] "etcd-minikube" [1b670871-586d-40e0-a650-46437e0eaba5] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0215 23:20:53.359969   15576 system_pods.go:61] "kube-apiserver-minikube" [599a1b20-2067-45bd-887d-85e3f054ab1c] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0215 23:20:53.359969   15576 system_pods.go:61] "kube-controller-manager-minikube" [ed58040d-3ae3-49de-accd-a577de6d6992] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0215 23:20:53.359969   15576 system_pods.go:61] "kube-scheduler-minikube" [4c8a9715-25ea-4b4c-8c88-a579b734761f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0215 23:20:53.359969   15576 system_pods.go:61] "storage-provisioner" [234e8d1e-c456-4c65-ad09-b4629de6276c] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I0215 23:20:53.359969   15576 system_pods.go:74] duration metric: took 31.1167ms to wait for pod list to return data ...
I0215 23:20:53.359969   15576 kubeadm.go:582] duration metric: took 1.8266103s to wait for: map[apiserver:true system_pods:true]
I0215 23:20:53.359969   15576 node_conditions.go:102] verifying NodePressure condition ...
I0215 23:20:53.374078   15576 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0215 23:20:53.374078   15576 node_conditions.go:123] node cpu capacity is 4
I0215 23:20:53.374078   15576 node_conditions.go:105] duration metric: took 14.1081ms to run NodePressure ...
I0215 23:20:53.374078   15576 start.go:241] waiting for startup goroutines ...
I0215 23:20:53.374078   15576 start.go:246] waiting for cluster config update ...
I0215 23:20:53.374078   15576 start.go:255] writing updated cluster config ...
I0215 23:20:53.389135   15576 ssh_runner.go:195] Run: rm -f paused
I0215 23:20:53.535574   15576 start.go:600] kubectl: 1.30.2, cluster: 1.32.0 (minor skew: 2)
I0215 23:20:53.538243   15576 out.go:201] 
W0215 23:20:53.542595   15576 out.go:270] ❗  C:\Program Files\Docker\Docker\resources\bin\kubectl.exe is version 1.30.2, which may have incompatibilities with Kubernetes 1.32.0.
I0215 23:20:53.548799   15576 out.go:177]     ▪ Want kubectl v1.32.0? Try 'minikube kubectl -- get pods -A'
I0215 23:20:53.560745   15576 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 15 17:20:29 minikube dockerd[1089]: time="2025-02-15T17:20:29.114854497Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Feb 15 17:20:29 minikube dockerd[1089]: time="2025-02-15T17:20:29.115225635Z" level=info msg="Daemon shutdown complete"
Feb 15 17:20:29 minikube dockerd[1089]: time="2025-02-15T17:20:29.115435456Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Feb 15 17:20:29 minikube systemd[1]: docker.service: Deactivated successfully.
Feb 15 17:20:29 minikube systemd[1]: Stopped Docker Application Container Engine.
Feb 15 17:20:29 minikube systemd[1]: Starting Docker Application Container Engine...
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.181627534Z" level=info msg="Starting up"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.184600334Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.231320148Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.265767224Z" level=info msg="Loading containers: start."
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.534407429Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.654278224Z" level=info msg="Loading containers: done."
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.681858906Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.681903711Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.681911712Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.681915912Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.681945915Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.681983919Z" level=info msg="Daemon has completed initialization"
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.770925593Z" level=info msg="API listen on /var/run/docker.sock"
Feb 15 17:20:29 minikube systemd[1]: Started Docker Application Container Engine.
Feb 15 17:20:29 minikube dockerd[1361]: time="2025-02-15T17:20:29.775554360Z" level=info msg="API listen on [::]:2376"
Feb 15 17:20:30 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Start docker client with request timeout 0s"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Hairpin mode is set to hairpin-veth"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Loaded network plugin cni"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Docker cri networking managed by network plugin cni"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Setting cgroupDriver cgroupfs"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Feb 15 17:20:30 minikube cri-dockerd[1638]: time="2025-02-15T17:20:30Z" level=info msg="Start cri-dockerd grpc backend"
Feb 15 17:20:30 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Feb 15 17:20:42 minikube cri-dockerd[1638]: time="2025-02-15T17:20:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa0f2c4116d1eb57c54cc7a076137c0bc8f5278f8fe1f9bc955a767081f1c007/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:42 minikube cri-dockerd[1638]: time="2025-02-15T17:20:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5cb2c46fde74caa18e5bfab2b6823743ad031bf6bca2d23ef93703c4a53c84af/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:43 minikube cri-dockerd[1638]: time="2025-02-15T17:20:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2ce20a9939a41e5802b03188747f339f885a3c86650f3ac1fc9ad17dc5917970/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:43 minikube cri-dockerd[1638]: time="2025-02-15T17:20:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6b2ffbbd610847fc8c684bf7692aacf0a66b552ec72f342537b38283196d55f6/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:56 minikube cri-dockerd[1638]: time="2025-02-15T17:20:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2da77d100680aac2f09ee651a12d3650663ae47b60e225f3557797bc244c4099/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:57 minikube cri-dockerd[1638]: time="2025-02-15T17:20:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/da4d534a7bd7d0cbb0b58f582ed7ac951e4357b28cd39becd1cf8bd8c8c774b2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:57 minikube cri-dockerd[1638]: time="2025-02-15T17:20:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/25dcb26311aea170919649a239ffd12842f5606dba7366b281866e54001b022a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Feb 15 17:20:57 minikube dockerd[1361]: time="2025-02-15T17:20:57.596877277Z" level=info msg="ignoring event" container=99b805ee60e1c09b20aee8019a6774e0702e846ae44c9ac51b972727a325b2f8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 15 17:21:01 minikube cri-dockerd[1638]: time="2025-02-15T17:21:01Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 15 17:21:36 minikube cri-dockerd[1638]: time="2025-02-15T17:21:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0822e5741f6b57fba52143495e0fd305f21fe6630cc22a208515d5e9b93b4c9/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 15 17:21:36 minikube cri-dockerd[1638]: time="2025-02-15T17:21:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2e01b05186d8ae377d2212aca48092a1c39bddbdecdab6f87d112d562096205f/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 15 17:21:37 minikube dockerd[1361]: time="2025-02-15T17:21:37.342399104Z" level=warning msg="reference for unknown type: " digest="sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f" remote="registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 15 17:21:48 minikube cri-dockerd[1638]: time="2025-02-15T17:21:48Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: d711cc0b3a79: Downloading [=============================>                     ]  14.93MB/25.36MB"
Feb 15 17:21:53 minikube cri-dockerd[1638]: time="2025-02-15T17:21:53Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 15 17:21:53 minikube dockerd[1361]: time="2025-02-15T17:21:53.809852306Z" level=info msg="ignoring event" container=a3f317dcb2b11afd53e44cc5cf1bdce14de994d9eb879a4c6e257e4b3e8007dc module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 15 17:21:53 minikube cri-dockerd[1638]: time="2025-02-15T17:21:53Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.4.4@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f"
Feb 15 17:21:54 minikube dockerd[1361]: time="2025-02-15T17:21:54.589225542Z" level=info msg="ignoring event" container=7363d8c14ffb9026c8c5a587d8e58a59d402958212113e42fd1663c96b2c190f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 15 17:21:55 minikube dockerd[1361]: time="2025-02-15T17:21:55.778027385Z" level=info msg="ignoring event" container=d0822e5741f6b57fba52143495e0fd305f21fe6630cc22a208515d5e9b93b4c9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 15 17:21:56 minikube dockerd[1361]: time="2025-02-15T17:21:56.913248931Z" level=info msg="ignoring event" container=2e01b05186d8ae377d2212aca48092a1c39bddbdecdab6f87d112d562096205f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 15 17:22:08 minikube cri-dockerd[1638]: time="2025-02-15T17:22:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2a8470aa8df4b04eb0245ddccf79180e4b4d5692877ee9d462a20c68af47f833/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 15 17:22:08 minikube dockerd[1361]: time="2025-02-15T17:22:08.858978619Z" level=warning msg="reference for unknown type: " digest="sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7" remote="registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"
Feb 15 17:22:20 minikube cri-dockerd[1638]: time="2025-02-15T17:22:20Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 77b70db29aee: Download complete "
Feb 15 17:22:30 minikube cri-dockerd[1638]: time="2025-02-15T17:22:30Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 1fadc9c48afa: Downloading [======================>                            ]  6.621MB/14.65MB"
Feb 15 17:22:40 minikube cri-dockerd[1638]: time="2025-02-15T17:22:40Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [==========================>                        ]  18.21MB/34.91MB"
Feb 15 17:22:50 minikube cri-dockerd[1638]: time="2025-02-15T17:22:50Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: 5070b1918f8e: Downloading [=========================================>         ]  28.91MB/34.91MB"
Feb 15 17:23:00 minikube cri-dockerd[1638]: time="2025-02-15T17:23:00Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: e39a383da384: Extracting [====================================>              ]  14.68MB/19.87MB"
Feb 15 17:23:05 minikube cri-dockerd[1638]: time="2025-02-15T17:23:05Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.11.3@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7"


==> container status <==
CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
ad045e5e0e36c       registry.k8s.io/ingress-nginx/controller@sha256:d56f135b6462cfc476447cfe564b83a45e8bb7da2774963b00d12161112270b7             2 minutes ago       Running             controller                0                   2a8470aa8df4b       ingress-nginx-controller-56d7c84fd4-tpx5r
7363d8c14ffb9       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   3 minutes ago       Exited              patch                     0                   2e01b05186d8a       ingress-nginx-admission-patch-9rmtt
a3f317dcb2b11       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:a9f03b34a3cbfbb26d103a14046ab2c5130a80c3d69d526ff8063d2b37b9fd3f   3 minutes ago       Exited              create                    0                   d0822e5741f6b       ingress-nginx-admission-create-h5qjw
4e9552cb16c53       6e38f40d628db                                                                                                                4 minutes ago       Running             storage-provisioner       1                   2da77d100680a       storage-provisioner
8145a5012fe46       c69fa2e9cbf5f                                                                                                                4 minutes ago       Running             coredns                   0                   25dcb26311aea       coredns-668d6bf9bc-v7rjh
32892aff8f4c5       040f9f8aac8cd                                                                                                                4 minutes ago       Running             kube-proxy                0                   da4d534a7bd7d       kube-proxy-vdgwm
99b805ee60e1c       6e38f40d628db                                                                                                                4 minutes ago       Exited              storage-provisioner       0                   2da77d100680a       storage-provisioner
0142692403e06       a9e7e6b294baf                                                                                                                5 minutes ago       Running             etcd                      0                   6b2ffbbd61084       etcd-minikube
07c9e839d12c5       a389e107f4ff1                                                                                                                5 minutes ago       Running             kube-scheduler            0                   2ce20a9939a41       kube-scheduler-minikube
5c87f92415865       8cab3d2a8bd0f                                                                                                                5 minutes ago       Running             kube-controller-manager   0                   5cb2c46fde74c       kube-controller-manager-minikube
219bbceba0881       c2e17b8d0f4a3                                                                                                                5 minutes ago       Running             kube-apiserver            0                   aa0f2c4116d1e       kube-apiserver-minikube


==> controller_ingress [ad045e5e0e36] <==
W0215 17:23:06.108115       8 client_config.go:659] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.
I0215 17:23:06.108830       8 main.go:205] "Creating API client" host="https://10.96.0.1:443"
I0215 17:23:06.121639       8 main.go:248] "Running in Kubernetes cluster" major="1" minor="32" git="v1.32.0" state="clean" commit="70d3cc986aa8221cd1dfb1121852688902d3bf53" platform="linux/amd64"
I0215 17:23:06.628926       8 main.go:101] "SSL fake certificate created" file="/etc/ingress-controller/ssl/default-fake-certificate.pem"
I0215 17:23:06.674115       8 ssl.go:535] "loading tls certificate" path="/usr/local/certificates/cert" key="/usr/local/certificates/key"
I0215 17:23:06.687494       8 nginx.go:271] "Starting NGINX Ingress controller"
I0215 17:23:06.704567       8 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"ingress-nginx-controller", UID:"5fb23780-e07b-454d-97e4-460385b8e40d", APIVersion:"v1", ResourceVersion:"461", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/ingress-nginx-controller
I0215 17:23:06.716098       8 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"tcp-services", UID:"2b938cb3-b44f-4ee7-bc4b-d575a03191fb", APIVersion:"v1", ResourceVersion:"462", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/tcp-services
I0215 17:23:06.716221       8 event.go:377] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"ingress-nginx", Name:"udp-services", UID:"fba60009-6772-4b0f-a7ce-4dcc9b1eaddd", APIVersion:"v1", ResourceVersion:"463", FieldPath:""}): type: 'Normal' reason: 'CREATE' ConfigMap ingress-nginx/udp-services
I0215 17:23:07.891065       8 nginx.go:317] "Starting NGINX process"
I0215 17:23:07.891120       8 leaderelection.go:254] attempting to acquire leader lease ingress-nginx/ingress-nginx-leader...
I0215 17:23:07.891607       8 nginx.go:337] "Starting validation webhook" address=":8443" certPath="/usr/local/certificates/cert" keyPath="/usr/local/certificates/key"
I0215 17:23:07.891810       8 controller.go:193] "Configuration changes detected, backend reload required"
I0215 17:23:07.909642       8 leaderelection.go:268] successfully acquired lease ingress-nginx/ingress-nginx-leader
I0215 17:23:07.909808       8 status.go:85] "New leader elected" identity="ingress-nginx-controller-56d7c84fd4-tpx5r"
I0215 17:23:07.929914       8 status.go:219] "POD is not ready" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tpx5r" node="minikube"
I0215 17:23:07.975587       8 controller.go:213] "Backend successfully reloaded"
I0215 17:23:07.975774       8 controller.go:224] "Initial sync, sleeping for 1 second"
I0215 17:23:07.975891       8 event.go:377] Event(v1.ObjectReference{Kind:"Pod", Namespace:"ingress-nginx", Name:"ingress-nginx-controller-56d7c84fd4-tpx5r", UID:"96ba35a6-0eb2-41e8-bffa-7b42ec0490e2", APIVersion:"v1", ResourceVersion:"607", FieldPath:""}): type: 'Normal' reason: 'RELOAD' NGINX reload triggered due to a change in configuration
-------------------------------------------------------------------------------
NGINX Ingress controller
  Release:       v1.11.3
  Build:         0106de65cfccb74405a6dfa7d9daffc6f0a6ef1a
  Repository:    https://github.com/kubernetes/ingress-nginx
  nginx version: nginx/1.25.5

-------------------------------------------------------------------------------



==> coredns [8145a5012fe4] <==
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:44651 - 50195 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" - - 0 6.004645602s
[ERROR] plugin/errors: 2 5434382074019690391.2210311041900018713. HINFO: read udp 10.244.0.2:58755->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:60982 - 1147 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" - - 0 6.003086497s
[ERROR] plugin/errors: 2 5434382074019690391.2210311041900018713. HINFO: read udp 10.244.0.2:35430->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:57583 - 33148 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" - - 0 4.003168316s
[ERROR] plugin/errors: 2 5434382074019690391.2210311041900018713. HINFO: read udp 10.244.0.2:47918->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:54298 - 7576 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" - - 0 2.001971397s
[ERROR] plugin/errors: 2 5434382074019690391.2210311041900018713. HINFO: read udp 10.244.0.2:46106->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:47025 - 629 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" - - 0 2.00849793s
[ERROR] plugin/errors: 2 5434382074019690391.2210311041900018713. HINFO: read udp 10.244.0.2:42918->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:33815 - 21167 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" - - 0 2.001272664s
[ERROR] plugin/errors: 2 5434382074019690391.2210311041900018713. HINFO: read udp 10.244.0.2:53710->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:60887 - 27338 "HINFO IN 5434382074019690391.2210311041900018713. udp 57 false 512" NXDOMAIN qr,rd,ra 57 1.5548197780000002s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_02_15T23_20_51_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 15 Feb 2025 17:20:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 15 Feb 2025 17:25:38 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 15 Feb 2025 17:23:24 +0000   Sat, 15 Feb 2025 17:20:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 15 Feb 2025 17:23:24 +0000   Sat, 15 Feb 2025 17:20:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 15 Feb 2025 17:23:24 +0000   Sat, 15 Feb 2025 17:20:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 15 Feb 2025 17:23:24 +0000   Sat, 15 Feb 2025 17:20:47 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7832440Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7832440Ki
  pods:               110
System Info:
  Machine ID:                 cb32560bcb4b4dcb81ca79d562ad1bbe
  System UUID:                cb32560bcb4b4dcb81ca79d562ad1bbe
  Boot ID:                    64820611-0717-4bc8-8f86-f9b56d4dc92b
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  ingress-nginx               ingress-nginx-controller-56d7c84fd4-tpx5r    100m (2%)     0 (0%)      90Mi (1%)        0 (0%)         4m13s
  kube-system                 coredns-668d6bf9bc-v7rjh                     100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     4m53s
  kube-system                 etcd-minikube                                100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         5m
  kube-system                 kube-apiserver-minikube                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         5m
  kube-system                 kube-controller-manager-minikube             200m (5%)     0 (0%)      0 (0%)           0 (0%)         4m59s
  kube-system                 kube-proxy-vdgwm                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m54s
  kube-system                 kube-scheduler-minikube                      100m (2%)     0 (0%)      0 (0%)           0 (0%)         4m59s
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m55s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%)  0 (0%)
  memory             260Mi (3%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                  From             Message
  ----     ------                             ----                 ----             -------
  Normal   Starting                           4m50s                kube-proxy       
  Normal   NodeHasSufficientMemory            5m7s (x8 over 5m7s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              5m7s (x8 over 5m7s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               5m7s (x7 over 5m7s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            5m7s                 kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           4m58s                kubelet          Starting kubelet.
  Warning  CgroupV1                           4m58s                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Warning  PossibleMemoryBackedVolumesOnDisk  4m58s                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeAllocatableEnforced            4m57s                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            4m57s                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              4m57s                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               4m57s                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     4m54s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.205135] /sbin/ldconfig.real: 
[  +0.000006] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.059905] FS-Cache: Duplicate cookie detected
[  +0.005765] FS-Cache: O-cookie c=00000040 [p=00000002 fl=222 nc=0 na=1]
[  +0.002420] FS-Cache: O-cookie d=00000000580a6f1a{9P.session} n=00000000e2043666
[  +0.001115] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.000365] FS-Cache: O-key=[10] '34323934393431373133'
[  +0.000009] FS-Cache: N-cookie c=00000041 [p=00000002 fl=2 nc=0 na=1]
[  +0.000002] FS-Cache: N-cookie d=00000000580a6f1a{9P.session} n=00000000be89f5b7
[  +0.000003] FS-Cache: N-key=[10] '34323934393431373133'
[  +0.010564] FS-Cache: Duplicate cookie detected
[  +0.000894] FS-Cache: O-cookie c=00000042 [p=00000002 fl=222 nc=0 na=1]
[  +0.000180] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000853] FS-Cache: O-cookie d=00000000580a6f1a{9P.session} n=00000000470a0e57
[  +0.000006] FS-Cache: O-key=[10] '34323934393431373135'
[  +0.000006] FS-Cache: N-cookie c=00000043 [p=00000002 fl=2 nc=0 na=1]
[  +0.000002] FS-Cache: N-cookie d=00000000580a6f1a{9P.session} n=000000009cd8d316
[  +0.000002] FS-Cache: N-key=[10] '34323934393431373135'
[  +0.005980] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.003512] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001657] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.003366] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001771] FS-Cache: Duplicate cookie detected
[  +0.000755] FS-Cache: O-cookie c=00000045 [p=00000002 fl=222 nc=0 na=1]
[  +0.000507] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000454] FS-Cache: O-cookie d=00000000580a6f1a{9P.session} n=000000000966332d
[  +0.000005] FS-Cache: O-key=[10] '34323934393431373137'
[  +0.000006] FS-Cache: N-cookie c=00000046 [p=00000002 fl=2 nc=0 na=1]
[  +0.000003] FS-Cache: N-cookie d=00000000580a6f1a{9P.session} n=00000000e0aa4d36
[  +0.003416] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000010] FS-Cache: N-key=[10] '34323934393431373137'
[  +0.004576] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.004283] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.987978] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002641] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.207199] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000070] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001499] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001766] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000275] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001650] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000065] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001233] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000152] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001186] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000121] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001095] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000138] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001111] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000096] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001242] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.002071] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.001701] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.349744] netlink: 'init': attribute type 4 has an invalid length.
[  +0.025771] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.017607] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[Feb15 17:19] hrtimer: interrupt took 1016935 ns
[Feb15 17:20] tmpfs: Unknown parameter 'noswap'
[  +9.852618] tmpfs: Unknown parameter 'noswap'


==> etcd [0142692403e0] <==
{"level":"info","ts":"2025-02-15T17:20:43.826400Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-02-15T17:20:43.826478Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-02-15T17:20:43.826520Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-02-15T17:20:43.826561Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-02-15T17:20:43.826629Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-02-15T17:20:43.859781Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-02-15T17:20:43.871087Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-02-15T17:20:43.920231Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-02-15T17:20:43.930258Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-02-15T17:20:43.931365Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"warn","ts":"2025-02-15T17:20:43.931536Z","logger":"raft","caller":"etcdserver/zap_raft.go:85","msg":"aec36adc501070cc cannot campaign at term 1 since there are still 1 pending configuration changes to apply"}
{"level":"info","ts":"2025-02-15T17:20:43.931866Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-15T17:20:43.932027Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-15T17:20:43.932150Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-02-15T17:20:43.932799Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-15T17:20:43.947756Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-02-15T17:20:43.948254Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-02-15T17:20:43.957035Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-02-15T17:20:43.961593Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-02-15T17:20:43.961816Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-02-15T17:20:43.962586Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-15T17:20:43.963187Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-02-15T17:20:44.927341Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-02-15T17:20:44.927396Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-02-15T17:20:44.927414Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-02-15T17:20:44.927433Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-02-15T17:20:44.927451Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-15T17:20:44.927465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-02-15T17:20:44.927482Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-02-15T17:20:44.938336Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-15T17:20:44.941935Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-02-15T17:20:44.946409Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-15T17:20:44.946833Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-02-15T17:20:44.947034Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-02-15T17:20:44.947051Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-02-15T17:20:44.947560Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-15T17:20:44.948302Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-02-15T17:20:44.948706Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-02-15T17:20:44.949416Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-02-15T17:20:44.958775Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-15T17:20:44.958900Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-15T17:20:44.958927Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-02-15T17:20:52.531074Z","caller":"traceutil/trace.go:171","msg":"trace[1145035181] transaction","detail":"{read_only:false; response_revision:306; number_of_response:1; }","duration":"109.247037ms","start":"2025-02-15T17:20:52.421779Z","end":"2025-02-15T17:20:52.531026Z","steps":["trace[1145035181] 'process raft request'  (duration: 84.766954ms)","trace[1145035181] 'compare'  (duration: 23.377476ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-15T17:20:56.014916Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"116.157122ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035300371412354 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/coredns-668d6bf9bc-v7rjh\" mod_revision:374 > success:<request_put:<key:\"/registry/pods/kube-system/coredns-668d6bf9bc-v7rjh\" value_size:3492 >> failure:<request_range:<key:\"/registry/pods/kube-system/coredns-668d6bf9bc-v7rjh\" > >>","response":"size:16"}
{"level":"info","ts":"2025-02-15T17:20:56.015142Z","caller":"traceutil/trace.go:171","msg":"trace[469454624] transaction","detail":"{read_only:false; response_revision:375; number_of_response:1; }","duration":"273.760319ms","start":"2025-02-15T17:20:55.741348Z","end":"2025-02-15T17:20:56.015108Z","steps":["trace[469454624] 'process raft request'  (duration: 156.034843ms)","trace[469454624] 'compare'  (duration: 115.981104ms)"],"step_count":2}
{"level":"info","ts":"2025-02-15T17:20:56.287590Z","caller":"traceutil/trace.go:171","msg":"trace[1063070944] transaction","detail":"{read_only:false; response_revision:377; number_of_response:1; }","duration":"542.564751ms","start":"2025-02-15T17:20:55.744974Z","end":"2025-02-15T17:20:56.287539Z","steps":["trace[1063070944] 'process raft request'  (duration: 542.520046ms)"],"step_count":1}
{"level":"info","ts":"2025-02-15T17:20:56.287621Z","caller":"traceutil/trace.go:171","msg":"trace[330755926] transaction","detail":"{read_only:false; response_revision:376; number_of_response:1; }","duration":"542.677261ms","start":"2025-02-15T17:20:55.744928Z","end":"2025-02-15T17:20:56.287605Z","steps":["trace[330755926] 'process raft request'  (duration: 542.452939ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-15T17:20:56.287687Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-15T17:20:55.744912Z","time spent":"542.745268ms","remote":"127.0.0.1:49516","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":704,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/coredns-668d6bf9bc.18247169e780fc3b\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/coredns-668d6bf9bc.18247169e780fc3b\" value_size:622 lease:8128035300371411645 >> failure:<>"}
{"level":"warn","ts":"2025-02-15T17:20:56.287687Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-15T17:20:55.744912Z","time spent":"542.726767ms","remote":"127.0.0.1:49904","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3737,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/replicasets/kube-system/coredns-668d6bf9bc\" mod_revision:366 > success:<request_put:<key:\"/registry/replicasets/kube-system/coredns-668d6bf9bc\" value_size:3677 >> failure:<request_range:<key:\"/registry/replicasets/kube-system/coredns-668d6bf9bc\" > >"}
{"level":"info","ts":"2025-02-15T17:20:56.295217Z","caller":"traceutil/trace.go:171","msg":"trace[1152979006] transaction","detail":"{read_only:false; response_revision:378; number_of_response:1; }","duration":"267.615815ms","start":"2025-02-15T17:20:56.027590Z","end":"2025-02-15T17:20:56.295206Z","steps":["trace[1152979006] 'process raft request'  (duration: 267.449698ms)"],"step_count":1}
{"level":"info","ts":"2025-02-15T17:20:56.295158Z","caller":"traceutil/trace.go:171","msg":"trace[56261419] linearizableReadLoop","detail":"{readStateIndex:389; appliedIndex:386; }","duration":"267.256679ms","start":"2025-02-15T17:20:56.027886Z","end":"2025-02-15T17:20:56.295142Z","steps":["trace[56261419] 'read index received'  (duration: 259.588325ms)","trace[56261419] 'applied index is now lower than readState.Index'  (duration: 7.667154ms)"],"step_count":2}
{"level":"warn","ts":"2025-02-15T17:20:56.295827Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"267.887842ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kube-system/coredns-668d6bf9bc-v7rjh\" limit:1 ","response":"range_response_count:1 size:3566"}
{"level":"info","ts":"2025-02-15T17:20:56.295863Z","caller":"traceutil/trace.go:171","msg":"trace[541338870] range","detail":"{range_begin:/registry/pods/kube-system/coredns-668d6bf9bc-v7rjh; range_end:; response_count:1; response_revision:378; }","duration":"268.000352ms","start":"2025-02-15T17:20:56.027853Z","end":"2025-02-15T17:20:56.295853Z","steps":["trace[541338870] 'agreement among raft nodes before linearized reading'  (duration: 267.712824ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-15T17:22:15.019867Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"285.342479ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035300371413010 > lease_revoke:<id:70cc950aa07937a1>","response":"size:29"}
{"level":"info","ts":"2025-02-15T17:22:15.168464Z","caller":"traceutil/trace.go:171","msg":"trace[1701911563] transaction","detail":"{read_only:false; response_revision:563; number_of_response:1; }","duration":"440.009708ms","start":"2025-02-15T17:22:14.728388Z","end":"2025-02-15T17:22:15.168397Z","steps":["trace[1701911563] 'process raft request'  (duration: 439.800906ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-15T17:22:15.168614Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-02-15T17:22:14.728358Z","time spent":"440.148308ms","remote":"127.0.0.1:49612","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:561 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-02-15T17:22:35.021401Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.843114ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128035300371413144 > lease_revoke:<id:70cc950aa0793832>","response":"size:29"}
{"level":"info","ts":"2025-02-15T17:22:35.021683Z","caller":"traceutil/trace.go:171","msg":"trace[411675496] transaction","detail":"{read_only:false; response_revision:579; number_of_response:1; }","duration":"182.694676ms","start":"2025-02-15T17:22:34.838974Z","end":"2025-02-15T17:22:35.021669Z","steps":["trace[411675496] 'process raft request'  (duration: 182.53156ms)"],"step_count":1}
{"level":"warn","ts":"2025-02-15T17:24:06.688722Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"267.920295ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-02-15T17:24:06.688800Z","caller":"traceutil/trace.go:171","msg":"trace[2053999773] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:679; }","duration":"268.006406ms","start":"2025-02-15T17:24:06.420780Z","end":"2025-02-15T17:24:06.688787Z","steps":["trace[2053999773] 'range keys from in-memory index tree'  (duration: 267.906794ms)"],"step_count":1}


==> kernel <==
 17:25:48 up 11 min,  0 users,  load average: 2.41, 2.65, 1.52
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [219bbceba088] <==
I0215 17:20:47.036503       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0215 17:20:47.036647       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0215 17:20:47.036663       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0215 17:20:47.036696       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0215 17:20:47.036503       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0215 17:20:47.037597       1 controller.go:142] Starting OpenAPI controller
I0215 17:20:47.037751       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0215 17:20:47.037774       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0215 17:20:47.037840       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0215 17:20:47.037915       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0215 17:20:47.037991       1 controller.go:90] Starting OpenAPI V3 controller
I0215 17:20:47.038095       1 naming_controller.go:294] Starting NamingConditionController
I0215 17:20:47.038294       1 establishing_controller.go:81] Starting EstablishingController
I0215 17:20:47.039273       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0215 17:20:47.039396       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0215 17:20:47.039493       1 crd_finalizer.go:269] Starting CRDFinalizer
I0215 17:20:47.042300       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0215 17:20:47.042360       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0215 17:20:47.042658       1 controller.go:78] Starting OpenAPI AggregationController
I0215 17:20:47.042732       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0215 17:20:47.048841       1 local_available_controller.go:156] Starting LocalAvailability controller
I0215 17:20:47.049044       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0215 17:20:47.049342       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0215 17:20:47.117006       1 shared_informer.go:320] Caches are synced for configmaps
I0215 17:20:47.144148       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0215 17:20:47.144552       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0215 17:20:47.144727       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0215 17:20:47.147224       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0215 17:20:47.147297       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0215 17:20:47.147747       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0215 17:20:47.150872       1 cache.go:39] Caches are synced for LocalAvailability controller
I0215 17:20:47.161016       1 shared_informer.go:320] Caches are synced for node_authorizer
I0215 17:20:47.161619       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0215 17:20:47.161940       1 policy_source.go:240] refreshing policies
I0215 17:20:47.212628       1 controller.go:615] quota admission added evaluator for: namespaces
I0215 17:20:47.214739       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0215 17:20:47.214781       1 aggregator.go:171] initial CRD sync complete...
I0215 17:20:47.214789       1 autoregister_controller.go:144] Starting autoregister controller
I0215 17:20:47.214797       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0215 17:20:47.214805       1 cache.go:39] Caches are synced for autoregister controller
E0215 17:20:47.223039       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0215 17:20:47.266187       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0215 17:20:48.052413       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0215 17:20:48.064382       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0215 17:20:48.064428       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0215 17:20:49.355948       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0215 17:20:49.463250       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0215 17:20:49.662114       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0215 17:20:49.678067       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0215 17:20:49.680568       1 controller.go:615] quota admission added evaluator for: endpoints
I0215 17:20:49.688820       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0215 17:20:50.073670       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0215 17:20:50.767582       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0215 17:20:50.799363       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0215 17:20:50.824125       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0215 17:20:54.876893       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0215 17:20:55.579227       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0215 17:21:35.337948       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller" clusterIPs={"IPv4":"10.98.93.0"}
I0215 17:21:35.400570       1 alloc.go:330] "allocated clusterIPs" service="ingress-nginx/ingress-nginx-controller-admission" clusterIPs={"IPv4":"10.101.116.82"}
I0215 17:21:35.463100       1 controller.go:615] quota admission added evaluator for: jobs.batch


==> kube-controller-manager [5c87f9241586] <==
I0215 17:20:54.722664       1 shared_informer.go:320] Caches are synced for TTL
I0215 17:20:54.726769       1 shared_informer.go:320] Caches are synced for deployment
I0215 17:20:54.729281       1 shared_informer.go:320] Caches are synced for disruption
I0215 17:20:54.729492       1 shared_informer.go:320] Caches are synced for garbage collector
I0215 17:20:54.729507       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0215 17:20:54.729518       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0215 17:20:54.729566       1 shared_informer.go:320] Caches are synced for cronjob
I0215 17:20:54.729723       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0215 17:20:54.733315       1 shared_informer.go:320] Caches are synced for TTL after finished
I0215 17:20:54.738480       1 shared_informer.go:320] Caches are synced for service account
I0215 17:20:54.748900       1 shared_informer.go:320] Caches are synced for attach detach
I0215 17:20:54.749109       1 shared_informer.go:320] Caches are synced for ephemeral
I0215 17:20:54.749175       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0215 17:20:54.752315       1 shared_informer.go:320] Caches are synced for ReplicationController
I0215 17:20:54.756231       1 shared_informer.go:320] Caches are synced for node
I0215 17:20:54.756390       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0215 17:20:54.756501       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0215 17:20:54.756516       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0215 17:20:54.756549       1 shared_informer.go:320] Caches are synced for cidrallocator
I0215 17:20:54.777400       1 shared_informer.go:320] Caches are synced for resource quota
I0215 17:20:54.777536       1 shared_informer.go:320] Caches are synced for garbage collector
I0215 17:20:54.796708       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0215 17:20:54.796751       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0215 17:20:54.796774       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0215 17:20:55.553493       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0215 17:20:56.289271       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="697.919027ms"
I0215 17:20:56.321997       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="32.671913ms"
I0215 17:20:56.322092       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="56.306µs"
I0215 17:20:56.335726       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="59.506µs"
I0215 17:20:58.587565       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="72.207µs"
I0215 17:21:00.437659       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="33.330577ms"
I0215 17:21:00.437855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="57.306µs"
I0215 17:21:01.562464       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0215 17:21:35.482526       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="0s"
I0215 17:21:35.504822       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="50.021237ms"
I0215 17:21:35.526025       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:35.527787       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="22.56792ms"
I0215 17:21:35.528121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="78.811µs"
I0215 17:21:35.561399       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="0s"
I0215 17:21:35.564225       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:35.603057       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="115.217µs"
I0215 17:21:35.607958       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:35.622621       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:35.640219       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:35.640760       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:35.706158       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:35.847902       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:54.568062       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:55.571593       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:55.979441       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:56.996582       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:57.011801       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-create" delay="1s"
I0215 17:21:57.058007       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:58.073098       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:21:58.087407       1 job_controller.go:598] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
I0215 17:22:23.398950       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0215 17:23:06.283907       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="1.123602ms"
I0215 17:23:17.382664       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="108.22622ms"
I0215 17:23:17.384859       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="ingress-nginx/ingress-nginx-controller-56d7c84fd4" duration="171.714µs"
I0215 17:23:24.076151       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [32892aff8f4c] <==
I0215 17:20:57.650596       1 server_linux.go:66] "Using iptables proxy"
I0215 17:20:57.938106       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0215 17:20:57.938323       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0215 17:20:57.973696       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0215 17:20:57.973945       1 server_linux.go:170] "Using iptables Proxier"
I0215 17:20:57.978872       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0215 17:20:57.987931       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0215 17:20:57.999963       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0215 17:20:58.000137       1 server.go:497] "Version info" version="v1.32.0"
I0215 17:20:58.000688       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0215 17:20:58.009618       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0215 17:20:58.020307       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0215 17:20:58.022213       1 config.go:199] "Starting service config controller"
I0215 17:20:58.022282       1 shared_informer.go:313] Waiting for caches to sync for service config
I0215 17:20:58.022312       1 config.go:105] "Starting endpoint slice config controller"
I0215 17:20:58.022320       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0215 17:20:58.024207       1 config.go:329] "Starting node config controller"
I0215 17:20:58.024593       1 shared_informer.go:313] Waiting for caches to sync for node config
I0215 17:20:58.122989       1 shared_informer.go:320] Caches are synced for service config
I0215 17:20:58.122989       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0215 17:20:58.124919       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [07c9e839d12c] <==
W0215 17:20:47.208690       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:47.208713       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.208832       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0215 17:20:47.208856       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.208952       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0215 17:20:47.208978       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0215 17:20:47.209107       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.209060       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0215 17:20:47.214693       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0215 17:20:47.214742       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.214903       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0215 17:20:47.214929       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.215018       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0215 17:20:47.215054       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
E0215 17:20:47.215076       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.208447       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:47.215130       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.208525       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0215 17:20:47.215155       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.212320       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:47.215197       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.212405       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:47.215225       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.212474       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0215 17:20:47.215265       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:47.215355       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0215 17:20:47.215375       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.044453       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0215 17:20:48.044522       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.046978       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:48.047041       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.247872       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0215 17:20:48.247938       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.253793       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0215 17:20:48.253851       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.323684       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0215 17:20:48.323968       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.326227       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0215 17:20:48.326489       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.363662       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0215 17:20:48.363825       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.394835       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0215 17:20:48.395078       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.457647       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0215 17:20:48.457716       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.461481       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0215 17:20:48.461541       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.499052       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:48.499293       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.648130       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:48.648203       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.660233       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0215 17:20:48.660295       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0215 17:20:48.732526       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:48.732618       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.796179       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0215 17:20:48.796376       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0215 17:20:48.808553       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0215 17:20:48.808861       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0215 17:20:50.284701       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Feb 15 17:20:51 minikube kubelet[2500]: I0215 17:20:51.359431    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 15 17:20:51 minikube kubelet[2500]: I0215 17:20:51.359472    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Feb 15 17:20:51 minikube kubelet[2500]: I0215 17:20:51.359506    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Feb 15 17:20:51 minikube kubelet[2500]: I0215 17:20:51.359569    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Feb 15 17:20:51 minikube kubelet[2500]: I0215 17:20:51.908322    2500 apiserver.go:52] "Watching apiserver"
Feb 15 17:20:51 minikube kubelet[2500]: I0215 17:20:51.954679    2500 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Feb 15 17:20:52 minikube kubelet[2500]: I0215 17:20:52.267100    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=3.267036416 podStartE2EDuration="3.267036416s" podCreationTimestamp="2025-02-15 17:20:49 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:52.221983429 +0000 UTC m=+1.543049852" watchObservedRunningTime="2025-02-15 17:20:52.267036416 +0000 UTC m=+1.588102839"
Feb 15 17:20:52 minikube kubelet[2500]: I0215 17:20:52.314301    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=4.31421351 podStartE2EDuration="4.31421351s" podCreationTimestamp="2025-02-15 17:20:48 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:52.267631874 +0000 UTC m=+1.588698297" watchObservedRunningTime="2025-02-15 17:20:52.31421351 +0000 UTC m=+1.635279933"
Feb 15 17:20:52 minikube kubelet[2500]: I0215 17:20:52.314527    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=3.314517539 podStartE2EDuration="3.314517539s" podCreationTimestamp="2025-02-15 17:20:49 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:52.31073017 +0000 UTC m=+1.631796593" watchObservedRunningTime="2025-02-15 17:20:52.314517539 +0000 UTC m=+1.635584062"
Feb 15 17:20:52 minikube kubelet[2500]: I0215 17:20:52.559062    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=4.559020247 podStartE2EDuration="4.559020247s" podCreationTimestamp="2025-02-15 17:20:48 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:52.3510837 +0000 UTC m=+1.672150123" watchObservedRunningTime="2025-02-15 17:20:52.559020247 +0000 UTC m=+1.880086670"
Feb 15 17:20:54 minikube kubelet[2500]: I0215 17:20:54.859612    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-p7xzn\" (UniqueName: \"kubernetes.io/projected/234e8d1e-c456-4c65-ad09-b4629de6276c-kube-api-access-p7xzn\") pod \"storage-provisioner\" (UID: \"234e8d1e-c456-4c65-ad09-b4629de6276c\") " pod="kube-system/storage-provisioner"
Feb 15 17:20:54 minikube kubelet[2500]: I0215 17:20:54.860360    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/234e8d1e-c456-4c65-ad09-b4629de6276c-tmp\") pod \"storage-provisioner\" (UID: \"234e8d1e-c456-4c65-ad09-b4629de6276c\") " pod="kube-system/storage-provisioner"
Feb 15 17:20:54 minikube kubelet[2500]: I0215 17:20:54.961540    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/92125fa2-9a03-4876-934a-45afcc2437d3-xtables-lock\") pod \"kube-proxy-vdgwm\" (UID: \"92125fa2-9a03-4876-934a-45afcc2437d3\") " pod="kube-system/kube-proxy-vdgwm"
Feb 15 17:20:54 minikube kubelet[2500]: I0215 17:20:54.961601    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-ggxnm\" (UniqueName: \"kubernetes.io/projected/92125fa2-9a03-4876-934a-45afcc2437d3-kube-api-access-ggxnm\") pod \"kube-proxy-vdgwm\" (UID: \"92125fa2-9a03-4876-934a-45afcc2437d3\") " pod="kube-system/kube-proxy-vdgwm"
Feb 15 17:20:54 minikube kubelet[2500]: I0215 17:20:54.961651    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/92125fa2-9a03-4876-934a-45afcc2437d3-kube-proxy\") pod \"kube-proxy-vdgwm\" (UID: \"92125fa2-9a03-4876-934a-45afcc2437d3\") " pod="kube-system/kube-proxy-vdgwm"
Feb 15 17:20:54 minikube kubelet[2500]: I0215 17:20:54.961673    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/92125fa2-9a03-4876-934a-45afcc2437d3-lib-modules\") pod \"kube-proxy-vdgwm\" (UID: \"92125fa2-9a03-4876-934a-45afcc2437d3\") " pod="kube-system/kube-proxy-vdgwm"
Feb 15 17:20:54 minikube kubelet[2500]: E0215 17:20:54.980615    2500 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 15 17:20:54 minikube kubelet[2500]: E0215 17:20:54.980690    2500 projected.go:194] Error preparing data for projected volume kube-api-access-p7xzn for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Feb 15 17:20:54 minikube kubelet[2500]: E0215 17:20:54.980987    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/234e8d1e-c456-4c65-ad09-b4629de6276c-kube-api-access-p7xzn podName:234e8d1e-c456-4c65-ad09-b4629de6276c nodeName:}" failed. No retries permitted until 2025-02-15 17:20:55.480964654 +0000 UTC m=+4.802031177 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-p7xzn" (UniqueName: "kubernetes.io/projected/234e8d1e-c456-4c65-ad09-b4629de6276c-kube-api-access-p7xzn") pod "storage-provisioner" (UID: "234e8d1e-c456-4c65-ad09-b4629de6276c") : configmap "kube-root-ca.crt" not found
Feb 15 17:20:55 minikube kubelet[2500]: E0215 17:20:55.090853    2500 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Feb 15 17:20:55 minikube kubelet[2500]: E0215 17:20:55.090893    2500 projected.go:194] Error preparing data for projected volume kube-api-access-ggxnm for pod kube-system/kube-proxy-vdgwm: configmap "kube-root-ca.crt" not found
Feb 15 17:20:55 minikube kubelet[2500]: E0215 17:20:55.091172    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/92125fa2-9a03-4876-934a-45afcc2437d3-kube-api-access-ggxnm podName:92125fa2-9a03-4876-934a-45afcc2437d3 nodeName:}" failed. No retries permitted until 2025-02-15 17:20:55.591117185 +0000 UTC m=+4.912183608 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-ggxnm" (UniqueName: "kubernetes.io/projected/92125fa2-9a03-4876-934a-45afcc2437d3-kube-api-access-ggxnm") pod "kube-proxy-vdgwm" (UID: "92125fa2-9a03-4876-934a-45afcc2437d3") : configmap "kube-root-ca.crt" not found
Feb 15 17:20:56 minikube kubelet[2500]: I0215 17:20:56.180655    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-96wch\" (UniqueName: \"kubernetes.io/projected/a289a1ff-e88b-4967-a15b-5d01310e4105-kube-api-access-96wch\") pod \"coredns-668d6bf9bc-v7rjh\" (UID: \"a289a1ff-e88b-4967-a15b-5d01310e4105\") " pod="kube-system/coredns-668d6bf9bc-v7rjh"
Feb 15 17:20:56 minikube kubelet[2500]: I0215 17:20:56.181528    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/a289a1ff-e88b-4967-a15b-5d01310e4105-config-volume\") pod \"coredns-668d6bf9bc-v7rjh\" (UID: \"a289a1ff-e88b-4967-a15b-5d01310e4105\") " pod="kube-system/coredns-668d6bf9bc-v7rjh"
Feb 15 17:20:57 minikube kubelet[2500]: I0215 17:20:57.577920    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=4.5778406050000005 podStartE2EDuration="4.577840605s" podCreationTimestamp="2025-02-15 17:20:53 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:57.57738546 +0000 UTC m=+6.898451883" watchObservedRunningTime="2025-02-15 17:20:57.577840605 +0000 UTC m=+6.898907028"
Feb 15 17:20:58 minikube kubelet[2500]: I0215 17:20:58.598868    2500 scope.go:117] "RemoveContainer" containerID="99b805ee60e1c09b20aee8019a6774e0702e846ae44c9ac51b972727a325b2f8"
Feb 15 17:20:58 minikube kubelet[2500]: I0215 17:20:58.651559    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-v7rjh" podStartSLOduration=3.651503579 podStartE2EDuration="3.651503579s" podCreationTimestamp="2025-02-15 17:20:55 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:58.59589381 +0000 UTC m=+7.916960233" watchObservedRunningTime="2025-02-15 17:20:58.651503579 +0000 UTC m=+7.972570002"
Feb 15 17:20:58 minikube kubelet[2500]: I0215 17:20:58.694104    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-vdgwm" podStartSLOduration=4.694082365 podStartE2EDuration="4.694082365s" podCreationTimestamp="2025-02-15 17:20:54 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-15 17:20:58.652668593 +0000 UTC m=+7.973735116" watchObservedRunningTime="2025-02-15 17:20:58.694082365 +0000 UTC m=+8.015148788"
Feb 15 17:20:59 minikube kubelet[2500]: I0215 17:20:59.612032    2500 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Feb 15 17:21:01 minikube kubelet[2500]: I0215 17:21:01.490379    2500 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Feb 15 17:21:01 minikube kubelet[2500]: I0215 17:21:01.508590    2500 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Feb 15 17:21:35 minikube kubelet[2500]: I0215 17:21:35.569146    2500 status_manager.go:890] "Failed to get status for pod" podUID="c51326c2-97c5-425e-ad76-28c51f8a3478" pod="ingress-nginx/ingress-nginx-admission-create-h5qjw" err="pods \"ingress-nginx-admission-create-h5qjw\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"ingress-nginx\": no relationship found between node 'minikube' and this object"
Feb 15 17:21:35 minikube kubelet[2500]: I0215 17:21:35.613091    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"webhook-cert\" (UniqueName: \"kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert\") pod \"ingress-nginx-controller-56d7c84fd4-tpx5r\" (UID: \"96ba35a6-0eb2-41e8-bffa-7b42ec0490e2\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tpx5r"
Feb 15 17:21:35 minikube kubelet[2500]: I0215 17:21:35.613172    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-s7vx6\" (UniqueName: \"kubernetes.io/projected/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-kube-api-access-s7vx6\") pod \"ingress-nginx-controller-56d7c84fd4-tpx5r\" (UID: \"96ba35a6-0eb2-41e8-bffa-7b42ec0490e2\") " pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tpx5r"
Feb 15 17:21:35 minikube kubelet[2500]: I0215 17:21:35.720223    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lmn2j\" (UniqueName: \"kubernetes.io/projected/c51326c2-97c5-425e-ad76-28c51f8a3478-kube-api-access-lmn2j\") pod \"ingress-nginx-admission-create-h5qjw\" (UID: \"c51326c2-97c5-425e-ad76-28c51f8a3478\") " pod="ingress-nginx/ingress-nginx-admission-create-h5qjw"
Feb 15 17:21:35 minikube kubelet[2500]: E0215 17:21:35.724391    2500 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 15 17:21:35 minikube kubelet[2500]: E0215 17:21:35.735266    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert podName:96ba35a6-0eb2-41e8-bffa-7b42ec0490e2 nodeName:}" failed. No retries permitted until 2025-02-15 17:21:36.235164084 +0000 UTC m=+45.560621825 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tpx5r" (UID: "96ba35a6-0eb2-41e8-bffa-7b42ec0490e2") : secret "ingress-nginx-admission" not found
Feb 15 17:21:35 minikube kubelet[2500]: I0215 17:21:35.825814    2500 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-mfvf5\" (UniqueName: \"kubernetes.io/projected/cd201e51-6268-45c3-8957-cd1d67a49052-kube-api-access-mfvf5\") pod \"ingress-nginx-admission-patch-9rmtt\" (UID: \"cd201e51-6268-45c3-8957-cd1d67a49052\") " pod="ingress-nginx/ingress-nginx-admission-patch-9rmtt"
Feb 15 17:21:36 minikube kubelet[2500]: E0215 17:21:36.237515    2500 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 15 17:21:36 minikube kubelet[2500]: E0215 17:21:36.238129    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert podName:96ba35a6-0eb2-41e8-bffa-7b42ec0490e2 nodeName:}" failed. No retries permitted until 2025-02-15 17:21:37.238035234 +0000 UTC m=+46.563492875 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tpx5r" (UID: "96ba35a6-0eb2-41e8-bffa-7b42ec0490e2") : secret "ingress-nginx-admission" not found
Feb 15 17:21:36 minikube kubelet[2500]: I0215 17:21:36.815190    2500 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2e01b05186d8ae377d2212aca48092a1c39bddbdecdab6f87d112d562096205f"
Feb 15 17:21:36 minikube kubelet[2500]: I0215 17:21:36.825676    2500 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d0822e5741f6b57fba52143495e0fd305f21fe6630cc22a208515d5e9b93b4c9"
Feb 15 17:21:37 minikube kubelet[2500]: E0215 17:21:37.248534    2500 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 15 17:21:37 minikube kubelet[2500]: E0215 17:21:37.248819    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert podName:96ba35a6-0eb2-41e8-bffa-7b42ec0490e2 nodeName:}" failed. No retries permitted until 2025-02-15 17:21:39.248754243 +0000 UTC m=+48.574211884 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tpx5r" (UID: "96ba35a6-0eb2-41e8-bffa-7b42ec0490e2") : secret "ingress-nginx-admission" not found
Feb 15 17:21:39 minikube kubelet[2500]: E0215 17:21:39.276024    2500 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 15 17:21:39 minikube kubelet[2500]: E0215 17:21:39.276226    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert podName:96ba35a6-0eb2-41e8-bffa-7b42ec0490e2 nodeName:}" failed. No retries permitted until 2025-02-15 17:21:43.276197717 +0000 UTC m=+52.601655358 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tpx5r" (UID: "96ba35a6-0eb2-41e8-bffa-7b42ec0490e2") : secret "ingress-nginx-admission" not found
Feb 15 17:21:43 minikube kubelet[2500]: E0215 17:21:43.280359    2500 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 15 17:21:43 minikube kubelet[2500]: E0215 17:21:43.280514    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert podName:96ba35a6-0eb2-41e8-bffa-7b42ec0490e2 nodeName:}" failed. No retries permitted until 2025-02-15 17:21:51.280491014 +0000 UTC m=+60.607623474 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tpx5r" (UID: "96ba35a6-0eb2-41e8-bffa-7b42ec0490e2") : secret "ingress-nginx-admission" not found
Feb 15 17:21:51 minikube kubelet[2500]: E0215 17:21:51.291991    2500 secret.go:189] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Feb 15 17:21:51 minikube kubelet[2500]: E0215 17:21:51.292121    2500 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert podName:96ba35a6-0eb2-41e8-bffa-7b42ec0490e2 nodeName:}" failed. No retries permitted until 2025-02-15 17:22:07.292101495 +0000 UTC m=+76.619233855 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/96ba35a6-0eb2-41e8-bffa-7b42ec0490e2-webhook-cert") pod "ingress-nginx-controller-56d7c84fd4-tpx5r" (UID: "96ba35a6-0eb2-41e8-bffa-7b42ec0490e2") : secret "ingress-nginx-admission" not found
Feb 15 17:21:56 minikube kubelet[2500]: I0215 17:21:56.109613    2500 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-lmn2j\" (UniqueName: \"kubernetes.io/projected/c51326c2-97c5-425e-ad76-28c51f8a3478-kube-api-access-lmn2j\") pod \"c51326c2-97c5-425e-ad76-28c51f8a3478\" (UID: \"c51326c2-97c5-425e-ad76-28c51f8a3478\") "
Feb 15 17:21:56 minikube kubelet[2500]: I0215 17:21:56.112718    2500 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/c51326c2-97c5-425e-ad76-28c51f8a3478-kube-api-access-lmn2j" (OuterVolumeSpecName: "kube-api-access-lmn2j") pod "c51326c2-97c5-425e-ad76-28c51f8a3478" (UID: "c51326c2-97c5-425e-ad76-28c51f8a3478"). InnerVolumeSpecName "kube-api-access-lmn2j". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 15 17:21:56 minikube kubelet[2500]: I0215 17:21:56.210566    2500 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-lmn2j\" (UniqueName: \"kubernetes.io/projected/c51326c2-97c5-425e-ad76-28c51f8a3478-kube-api-access-lmn2j\") on node \"minikube\" DevicePath \"\""
Feb 15 17:21:56 minikube kubelet[2500]: I0215 17:21:56.603926    2500 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="d0822e5741f6b57fba52143495e0fd305f21fe6630cc22a208515d5e9b93b4c9"
Feb 15 17:21:57 minikube kubelet[2500]: I0215 17:21:57.119475    2500 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-mfvf5\" (UniqueName: \"kubernetes.io/projected/cd201e51-6268-45c3-8957-cd1d67a49052-kube-api-access-mfvf5\") pod \"cd201e51-6268-45c3-8957-cd1d67a49052\" (UID: \"cd201e51-6268-45c3-8957-cd1d67a49052\") "
Feb 15 17:21:57 minikube kubelet[2500]: I0215 17:21:57.128865    2500 operation_generator.go:780] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/cd201e51-6268-45c3-8957-cd1d67a49052-kube-api-access-mfvf5" (OuterVolumeSpecName: "kube-api-access-mfvf5") pod "cd201e51-6268-45c3-8957-cd1d67a49052" (UID: "cd201e51-6268-45c3-8957-cd1d67a49052"). InnerVolumeSpecName "kube-api-access-mfvf5". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Feb 15 17:21:57 minikube kubelet[2500]: I0215 17:21:57.220481    2500 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-mfvf5\" (UniqueName: \"kubernetes.io/projected/cd201e51-6268-45c3-8957-cd1d67a49052-kube-api-access-mfvf5\") on node \"minikube\" DevicePath \"\""
Feb 15 17:21:57 minikube kubelet[2500]: I0215 17:21:57.674140    2500 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2e01b05186d8ae377d2212aca48092a1c39bddbdecdab6f87d112d562096205f"
Feb 15 17:22:08 minikube kubelet[2500]: I0215 17:22:08.166253    2500 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2a8470aa8df4b04eb0245ddccf79180e4b4d5692877ee9d462a20c68af47f833"
Feb 15 17:23:06 minikube kubelet[2500]: I0215 17:23:06.255612    2500 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="ingress-nginx/ingress-nginx-controller-56d7c84fd4-tpx5r" podStartSLOduration=34.09032057 podStartE2EDuration="1m31.255589915s" podCreationTimestamp="2025-02-15 17:21:35 +0000 UTC" firstStartedPulling="2025-02-15 17:22:08.276845603 +0000 UTC m=+77.603977963" lastFinishedPulling="2025-02-15 17:23:05.437210654 +0000 UTC m=+134.769247308" observedRunningTime="2025-02-15 17:23:06.254023374 +0000 UTC m=+135.586060028" watchObservedRunningTime="2025-02-15 17:23:06.255589915 +0000 UTC m=+135.587626569"


==> storage-provisioner [4e9552cb16c5] <==
I0215 17:20:58.942601       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0215 17:20:58.961372       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0215 17:20:58.961530       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0215 17:20:58.973322       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0215 17:20:58.973561       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_88650af9-5f01-462d-a708-21b5ed189801!
I0215 17:20:58.974460       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"d44297ce-81c1-4ebc-b044-c84b0fe85792", APIVersion:"v1", ResourceVersion:"409", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_88650af9-5f01-462d-a708-21b5ed189801 became leader
I0215 17:20:59.073825       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_88650af9-5f01-462d-a708-21b5ed189801!


==> storage-provisioner [99b805ee60e1] <==
I0215 17:20:57.479519       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0215 17:20:57.503727       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

